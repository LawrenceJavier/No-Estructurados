{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Utilización de varias técnicas para realizar sentiment analysis:\n","- RNN\n","- Word embedding\n","- Transfer learning\n","\n","Autores:\n","- Lawrence Javier Minguillán Van Kapel\n","- Alberto Sánchez Bonastre"]},{"cell_type":"markdown","metadata":{},"source":["## Cargamos los datos de google collab"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2100,"status":"ok","timestamp":1714502073883,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"OJ5FZCdxPI5-","outputId":"8b26a68c-7539-4cb3-fa78-19e000a67dc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{},"source":["Almacenamiento de nuestro dataset en la variable data"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1714502081351,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"5hQTiJZlM4GL","outputId":"e48b8507-408b-4480-b7ed-ea55f0cb25fd"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"data\",\n  \"rows\": 5842,\n  \"fields\": [\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5322,\n        \"samples\": [\n          \"It is now the leading private road ambulance service company in Finland .\",\n          \"Finnish silicon wafers manufacturer Okmetic Oyj said it swung to a net profit of 4.9 mln euro $ 6.3 mln in the first nine months of 2006 from a net loss of 1.8 mln euro $ 2.3 mln a year earlier .\",\n          \"$GILD  is expanding its research facilities...keeping up with the pace of innovation  https://t.co/uOE7FJ4LOP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"data"},"text/html":["\n","  <div id=\"df-cca2e51c-fa8c-4f89-a2b0-b6ef96c2bbdd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The GeoSolutions technology will leverage Bene...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>For the last quarter of 2010 , Componenta 's n...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>According to the Finnish-Russian Chamber of Co...</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The Swedish buyout firm has sold its remaining...</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cca2e51c-fa8c-4f89-a2b0-b6ef96c2bbdd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cca2e51c-fa8c-4f89-a2b0-b6ef96c2bbdd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cca2e51c-fa8c-4f89-a2b0-b6ef96c2bbdd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0a58e05b-f593-4a28-b9a3-d40be4d5b4bb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a58e05b-f593-4a28-b9a3-d40be4d5b4bb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0a58e05b-f593-4a28-b9a3-d40be4d5b4bb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                                            Sentence Sentiment\n","0  The GeoSolutions technology will leverage Bene...  positive\n","1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n","2  For the last quarter of 2010 , Componenta 's n...  positive\n","3  According to the Finnish-Russian Chamber of Co...   neutral\n","4  The Swedish buyout firm has sold its remaining...   neutral"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/Comillas-ICAI/data.csv\")\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"w1-DaS_YOBlE"},"source":["# RNN for sentiment analysis"]},{"cell_type":"markdown","metadata":{},"source":["En este modelo de análisis de sentimientos, hemos seguido los siguientes pasos:\n","\n","1. Preparación de los datos: Comenzamos importando las bibliotecas necesarias, incluyendo pandas y numpy para el manejo de datos, así como las clases relacionadas con la construcción y entrenamiento de modelos de redes neuronales recurrentes (RNN) utilizando TensorFlow.\n","2. Carga y limpieza de datos: Importamos nuestros datos desde un archivo CSV y los cargamos en un DataFrame. Luego, eliminamos cualquier fila que tenga valores faltantes utilizando el método dropna.\n","3. Codificación de etiquetas de sentimiento: Utilizamos LabelEncoder para convertir las etiquetas de sentimiento de texto en valores numéricos. Esto es necesario para que el modelo pueda trabajar con ellas durante el entrenamiento.\n","4. División de datos: Dividimos nuestros datos en conjuntos de entrenamiento y prueba utilizando train_test_split de scikit-learn. Esto nos permite evaluar la capacidad de generalización del modelo.\n","5. Tokenización y secuenciación de texto: Utilizamos Tokenizer y pad_sequences de TensorFlow para convertir las secuencias de texto en secuencias de números enteros, preparándolas para ser procesadas por la red neuronal.\n","6. Construcción del modelo de red neuronal recurrente (RNN): Creamos un modelo secuencial utilizando Sequential() de TensorFlow. Este modelo consta de una capa de embedding, una capa LSTM y una capa densa. La capa de embedding convierte los números enteros en vectores de números reales, la capa LSTM procesa secuencias de entrada y la capa densa realiza la clasificación final.\n","7. Compilación del modelo: Compilamos el modelo especificando el optimizador, la función de pérdida y las métricas que se utilizarán durante el entrenamiento. En este caso, utilizamos el optimizador Adam y la función de pérdida de entropía cruzada binaria.\n","7. Entrenamiento del modelo: Entrenamos el modelo utilizando el conjunto de entrenamiento, con validación cruzada para monitorear el rendimiento. Utilizamos EarlyStopping para detener el entrenamiento si la pérdida en el conjunto de validación deja de disminuir, lo que ayuda a evitar el sobreajuste.\n","8. Evaluación del modelo: Finalmente, evaluamos el rendimiento del modelo utilizando el conjunto de prueba y mostramos la precisión obtenida. Esto nos da una idea de qué tan bien el modelo puede predecir los sentimientos en datos que no ha visto durante el entrenamiento."]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":838222,"status":"ok","timestamp":1714505977004,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"rAmYeKAmMKXI","outputId":"fbae3c34-0623-4a34-b185-ca57a5fcd806"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","117/117 [==============================] - 59s 432ms/step - loss: -1.6232 - accuracy: 0.5452 - val_loss: -2.9213 - val_accuracy: 0.5016\n","Epoch 2/20\n","117/117 [==============================] - 48s 406ms/step - loss: -3.1987 - accuracy: 0.5455 - val_loss: -4.1350 - val_accuracy: 0.5016\n","Epoch 3/20\n","117/117 [==============================] - 41s 353ms/step - loss: -4.2576 - accuracy: 0.5455 - val_loss: -5.2781 - val_accuracy: 0.5016\n","Epoch 4/20\n","117/117 [==============================] - 42s 361ms/step - loss: -5.2997 - accuracy: 0.5455 - val_loss: -6.4481 - val_accuracy: 0.5016\n","Epoch 5/20\n","117/117 [==============================] - 41s 350ms/step - loss: -6.3755 - accuracy: 0.5455 - val_loss: -7.6390 - val_accuracy: 0.5016\n","Epoch 6/20\n","117/117 [==============================] - 41s 347ms/step - loss: -7.4359 - accuracy: 0.5455 - val_loss: -8.7988 - val_accuracy: 0.5016\n","Epoch 7/20\n","117/117 [==============================] - 40s 346ms/step - loss: -8.5199 - accuracy: 0.5455 - val_loss: -10.0195 - val_accuracy: 0.5016\n","Epoch 8/20\n","117/117 [==============================] - 42s 360ms/step - loss: -9.5917 - accuracy: 0.5455 - val_loss: -11.1025 - val_accuracy: 0.5016\n","Epoch 9/20\n","117/117 [==============================] - 40s 342ms/step - loss: -10.7565 - accuracy: 0.5455 - val_loss: -12.2292 - val_accuracy: 0.5016\n","Epoch 10/20\n","117/117 [==============================] - 40s 345ms/step - loss: -14.1964 - accuracy: 0.5455 - val_loss: -11.7107 - val_accuracy: 0.5016\n","Epoch 11/20\n","117/117 [==============================] - 40s 341ms/step - loss: -18.7390 - accuracy: 0.5455 - val_loss: -13.8664 - val_accuracy: 0.5016\n","Epoch 12/20\n","117/117 [==============================] - 40s 339ms/step - loss: -21.0874 - accuracy: 0.5455 - val_loss: -15.9031 - val_accuracy: 0.5016\n","Epoch 13/20\n","117/117 [==============================] - 40s 343ms/step - loss: -23.2795 - accuracy: 0.5455 - val_loss: -18.2607 - val_accuracy: 0.5016\n","Epoch 14/20\n","117/117 [==============================] - 41s 353ms/step - loss: -25.4769 - accuracy: 0.5441 - val_loss: -19.9600 - val_accuracy: 0.5016\n","Epoch 15/20\n","117/117 [==============================] - 40s 343ms/step - loss: -27.8336 - accuracy: 0.5455 - val_loss: -21.6879 - val_accuracy: 0.5005\n","Epoch 16/20\n","117/117 [==============================] - 39s 337ms/step - loss: -31.2387 - accuracy: 0.5460 - val_loss: -25.0891 - val_accuracy: 0.4920\n","Epoch 17/20\n","117/117 [==============================] - 40s 338ms/step - loss: -34.9542 - accuracy: 0.5484 - val_loss: -28.0225 - val_accuracy: 0.4877\n","Epoch 18/20\n","117/117 [==============================] - 40s 340ms/step - loss: -38.5478 - accuracy: 0.5543 - val_loss: -29.6609 - val_accuracy: 0.4834\n","Epoch 19/20\n","117/117 [==============================] - 39s 335ms/step - loss: -41.4049 - accuracy: 0.5522 - val_loss: -32.4087 - val_accuracy: 0.4834\n","Epoch 20/20\n","117/117 [==============================] - 42s 356ms/step - loss: -44.0365 - accuracy: 0.5562 - val_loss: -35.1683 - val_accuracy: 0.4813\n","37/37 [==============================] - 1s 30ms/step - loss: -32.1986 - accuracy: 0.5218\n","Precisión del modelo: 0.5218135118484497\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Leer el archivo CSV\n","df = data\n","df.dropna(inplace=True)\n","\n","# Sentence,Sentiment\n","# Codificar las etiquetas de sentimiento\n","label_encoder = LabelEncoder()\n","df['Sentiment'] = label_encoder.fit_transform(df['Sentiment'])\n","\n","# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['Sentiment'], test_size=0.2, random_state=42)\n","\n","# Tokenización y secuenciación de texto\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X_train)\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","max_len = 100  # Longitud máxima de la secuencia\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n","\n","# Crear el modelo de red neuronal recurrente\n","embedding_dim = 100\n","model = Sequential()\n","model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n","model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(units=1, activation='sigmoid'))\n","\n","# Compilar el modelo\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Entrenar el modelo\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","history = model.fit(X_train_pad, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n","\n","# Evaluar el modelo\n","loss, accuracy = model.evaluate(X_test_pad, y_test)\n","print(\"Precisión del modelo:\", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"zzMy05nxDE68"},"source":["*   Probamos el modelo con 10 epocs -> produce 34% de precisión - 10 minutos\n","*   Probamos el modelo con 20 epocs -> produce 52% de precisión - 20 minutos\n","*   Probamos el modelo con 40 epocs -> produce 54% de precisión - 40 minutos\n","\n","Tambien se ha cambiado el tamaño de los batches del entreanmiento reduciendolos a 4, 6 y 8 aumnetando las epocs a 50 pero el resultado final deja una precisión resultante de 54% y un tiempo de procesamiento de aproximadamente 1 hora. Por lo que en comparativa con los modelos determinamos que el \"codo\" de tiempo de procesamiento vs accuracy recae en el modelo entrenado con 20 epocas y un batch size de 32."]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":228,"status":"ok","timestamp":1714505995825,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"6rFMFu5QNx5T"},"outputs":[],"source":["# Guardamos el modelo\n","model.save('./model_rnn.keras')"]},{"cell_type":"markdown","metadata":{},"source":["Como la precisión de nuestro modelo actual es baja, alrededor del 54%, una estrategia que podemos utilizar para mejorarla es incorporar word embeddings pre-entrenados. Estos embeddings son representaciones vectoriales de palabras que capturan relaciones semánticas y contextuales entre palabras en un espacio vectorial. Utilizar word embeddings pre-entrenados nos permite aprovechar el conocimiento lingüístico aprendido de grandes corpus de texto y aplicarlo a nuestra tarea específica de análisis de sentimientos."]},{"cell_type":"markdown","metadata":{"id":"8Og-RHQXNzgn"},"source":["# Word embeddings pre-entrenados - GloVe embeddings en un modelo LSTM"]},{"cell_type":"markdown","metadata":{},"source":["Ahora realizaremos un análisis de similitud de coseno utilizando vectores de palabras pre-entrenados, específicamente el conjunto de vectores GloVe. Los pasos que se han seguido son los siguientes:\n","\n","1. Carga de vectores de palabras pre-entrenados: Se carga un archivo de vectores de palabras pre-entrenados (en este caso, se utiliza el archivo 'glove.42B.300d.txt') que contiene las representaciones vectoriales de palabras en un espacio de alta dimensionalidad.\n","2. Creación de un índice de embeddings: Se crea un diccionario que mapea cada palabra a su vector de embedding correspondiente. Esto permite acceder rápidamente a los vectores de palabras durante el procesamiento."]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":135409,"status":"ok","timestamp":1714508857032,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"_IQc_5O8QWz_"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","embedding_file = '/content/drive/MyDrive/Comillas-ICAI/glove.42B.300d.txt'\n","\n","embedding_index = {}\n","with open(embedding_file, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embedding_index[word] = coefs\n","\n","# Función para obtener el vector promedio de una frase\n","def get_sentence_vector(sentence):\n","    words = sentence.split()\n","    sentence_vector = np.zeros((len(words), 300))  # Tamaño del vector de palabra GloVe\n","    for i, word in enumerate(words):\n","        if word in embedding_index:\n","            sentence_vector[i] = embedding_index[word]\n","    return np.mean(sentence_vector, axis=0)"]},{"cell_type":"markdown","metadata":{},"source":["3. Definición de una función para obtener el vector de una frase: Se define una función llamada get_sentence_vector(sentence) que toma una frase como entrada y devuelve su vector de embedding promedio. Para ello, se divide la frase en palabras y se busca cada palabra en el índice de embeddings para obtener su vector correspondiente. Se calcula el vector promedio de todas las palabras en la frase."]},{"cell_type":"code","execution_count":106,"metadata":{"executionInfo":{"elapsed":30323,"status":"ok","timestamp":1714508951236,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"n6hUHRY6r4vy"},"outputs":[],"source":["import pickle\n","\n","with open('./embedding_index.pickle', 'wb') as handle:\n","    pickle.dump(embedding_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{},"source":["4. Guardado del índice de embeddings en un archivo pickle: El índice de embeddings se guarda en un archivo pickle llamado 'embedding_index.pickle' para su posterior uso.\n","5. División de datos en conjuntos de entrenamiento y prueba: Se dividen los datos de entrada en conjuntos de entrenamiento y prueba utilizando la función train_test_split de scikit-learn.\n","6. Obtención de vectores de características para frases de entrenamiento y prueba: Se utiliza la función get_sentence_vector para obtener los vectores de características correspondientes a las frases de entrenamiento y prueba.\n","7. Entrenamiento de un clasificador (por ejemplo, SVM): Se entrena un clasificador, en este caso un Support Vector Classifier (SVC) con un kernel lineal, utilizando los vectores de características de las frases de entrenamiento y sus respectivas etiquetas de sentimiento.\n","8. Evaluación del clasificador en el conjunto de prueba: Se evalúa el rendimiento del clasificador en el conjunto de prueba calculando su precisión, que es la proporción de predicciones correctas sobre el total de predicciones."]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3941,"status":"ok","timestamp":1714508896502,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"3DmwLzkAN6eq","outputId":"05051ecd-d36a-4a7e-82d0-890ac8ca8536"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precisión del clasificador: 0.7125748502994012\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","X = data['Sentence']\n","y = data['Sentiment']\n","\n","# Dividir datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Obtener vectores de características para frases de entrenamiento y prueba\n","X_train_vectors = np.array([get_sentence_vector(sentence) for sentence in X_train])\n","X_test_vectors = np.array([get_sentence_vector(sentence) for sentence in X_test])\n","\n","# Entrenar un clasificador (por ejemplo, SVM)\n","clf = SVC(kernel='linear')\n","clf.fit(X_train_vectors, y_train)\n","\n","# Evaluar el clasificador en el conjunto de prueba\n","accuracy = clf.score(X_test_vectors, y_test)\n","print(\"Precisión del clasificador:\", accuracy)\n"]},{"cell_type":"markdown","metadata":{},"source":["Mediante el uso de word embeddings pre-entrenados, hemos logrado mejorar significativamente la precisión de nuestro modelo de análisis de sentimientos. Mientras que nuestro modelo anterior, basado en redes neuronales recurrentes (RNN), alcanzó una precisión del 54%, la incorporación de word embeddings nos ha permitido elevar esta métrica a un impresionante 71%."]},{"cell_type":"markdown","metadata":{},"source":["Mostramos un ejemplo del vector embedding de la palabra 'lawrence'."]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1714508899496,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"gPWm3qsLWeil","outputId":"4df8ecea-2f09-4849-e886-aa1e9ef39784"},"outputs":[{"name":"stdout","output_type":"stream","text":["Embedding vector for \"lawrence\": [-2.9427e-01 -5.7970e-02 -1.5971e-02  3.9954e-01 -6.2847e-01 -4.1986e-02\n"," -6.4807e-01 -5.7400e-01  5.4430e-02  1.4884e-01  1.3946e-01  2.3571e-01\n","  1.7424e-01 -9.1451e-02 -6.7695e-01 -2.6709e-01  2.8099e-01  9.1666e-02\n","  6.0602e-02 -4.2523e-01 -3.0175e-01 -3.4897e-02 -2.9880e-01 -3.2512e-01\n"," -1.2465e-01  2.9856e-01 -1.4872e-01 -1.9790e-01 -2.2669e-01  3.3482e-01\n","  4.0580e-01  5.5613e-02  1.5809e-01 -1.0308e-01  1.0452e-01 -1.3513e-01\n","  1.5482e-01  4.3143e-01 -9.6263e-02 -3.6093e-01 -1.4344e-01  1.5833e-01\n","  5.6850e-01 -3.8225e-01  2.2130e-01  4.2912e-02 -5.2333e-01 -1.3488e-02\n","  5.6025e-01  2.6849e-01  2.8533e-01 -2.7855e-01 -2.9604e-02 -1.1529e-01\n","  1.0453e-01  3.0762e-01  2.5936e-01  1.8043e-01 -1.6598e-01  3.1576e-01\n"," -2.3928e-01  1.7130e-01  5.7337e-01 -2.9780e-01 -2.1183e-01 -3.1928e-01\n","  1.3632e-01 -7.3382e-02  3.3463e-01 -6.0898e-01  2.7280e-01 -4.5832e-01\n","  1.8715e-01 -4.0205e-02  1.5931e-01  2.5965e-01 -3.7205e-01 -4.2004e-01\n"," -1.4238e-01  6.5839e-02 -5.3795e-01 -1.1699e+00  2.4534e-02  1.4199e-01\n","  8.0435e-03 -2.3298e-01  1.2264e-01 -6.4576e-01 -1.9993e-02  2.7685e-01\n"," -2.4059e-01  1.2893e-01  3.3641e-02  9.4158e-02  1.0846e-01 -9.3458e-02\n"," -1.3839e+00 -3.4030e-01 -4.9263e-01 -3.0204e-01 -2.2649e-01 -2.1118e-01\n"," -9.8853e-02  8.9577e-02  1.3411e-01 -1.8157e-01  1.3122e-01 -1.3981e-01\n","  8.2003e-02  4.8827e-02  2.1167e-01  2.9869e-01  2.1485e-01 -2.0043e-01\n","  4.5286e-01  3.1009e-01  1.2801e-01 -2.1245e-01  5.4117e-01  8.4787e-02\n","  2.0981e-02  2.0731e-01 -4.5440e-02  2.6700e-01  1.2498e-01 -6.4703e-02\n"," -2.0152e-01 -5.6914e-02 -8.8578e-02  1.0546e-01  3.5032e-01  3.8791e-02\n","  2.1006e-01 -3.2918e-02  3.7113e-01  8.3900e-02  3.6985e-01  1.5848e-02\n","  2.1945e-01  7.7545e-01  8.3852e-02 -6.8251e-01 -1.2372e-01  5.2944e-01\n","  6.8956e-03 -3.8723e-01  1.3761e-01 -2.8669e-01 -8.2333e-02 -7.0649e-02\n","  1.4704e-01 -1.3885e-01  6.3531e-02 -4.5374e-01 -4.6892e-01 -6.0122e-02\n"," -7.4878e-02  4.6552e-01 -2.6893e-01 -6.0565e-02 -2.1638e-01 -3.4442e-01\n","  4.8506e-02  1.0640e-01 -8.5274e-02  1.3283e-01  4.6943e-04  2.4049e-01\n","  3.1822e-01 -1.4915e-01  1.7583e-01 -1.6393e-01 -4.5147e-01  1.6409e-01\n"," -2.5735e-01  3.2571e-01 -2.2113e-01 -1.7126e-01  4.3610e-01 -5.1567e-01\n"," -2.6945e-01  1.3153e-02 -6.7119e-02 -2.0984e-01 -1.6226e-01 -4.3528e-01\n","  9.3217e-02  2.0485e-01 -4.3544e-01  3.6790e-02 -4.4048e-01 -6.4556e-01\n","  2.2347e-01  5.6316e-02  1.7158e-01 -4.9098e-01 -3.1373e-01 -2.8417e-01\n"," -1.4979e-01  3.8501e-01 -1.8509e-01 -2.9212e-01 -1.1948e-01 -1.4405e-01\n"," -5.4110e-02 -1.1579e-01  6.7876e-01  3.1846e-01 -1.4529e-01 -3.2987e-01\n","  2.1364e-01 -2.0218e-01 -6.9553e-01 -2.2405e-01 -1.1214e-01 -1.0652e-01\n","  1.4376e-01 -7.0651e-02 -7.7702e-02 -1.7013e-01  4.9135e-02  1.1814e-02\n","  1.5475e-02  1.9308e-01 -2.9040e+00 -8.2867e-03 -3.5507e-01  8.0353e-02\n","  1.4443e-01 -4.0877e-01 -5.2040e-01  1.1899e-01  2.8739e-01 -1.5206e-01\n"," -1.4268e-01 -2.9122e-02 -2.3395e-01  2.8162e-01  1.7990e-01 -8.1173e-01\n","  1.1048e-01  9.3926e-02 -3.3903e-01 -4.1108e-02  5.3738e-02  2.9997e-01\n","  5.6925e-01  1.4673e-01 -3.7093e-02  1.4238e-01 -2.3411e-01  4.7394e-01\n","  6.0911e-03  3.2154e-04 -1.7484e-01  4.6571e-01 -9.1270e-02  2.6995e-01\n","  3.0410e-01 -2.0124e-01  3.4114e-01  1.5452e-01  2.6870e-02  5.9373e-01\n","  7.1591e-02  2.9959e-01  6.9301e-01 -1.2632e-01 -8.7483e-02  1.4522e-01\n"," -1.1520e-01 -2.8171e-01 -4.4787e-01  4.4436e-01  2.7786e-01 -9.2439e-03\n","  1.2230e-01 -7.0059e-01 -3.0752e-02  5.2579e-02 -1.0629e-01  1.9138e-01\n"," -1.2623e-01 -5.6725e-02 -1.0428e-01  2.3997e-01 -1.7533e-01  3.9627e-01\n"," -1.3080e-01  4.4611e-01  2.2359e-01 -3.2299e-01 -3.6454e-02  8.6503e-03\n","  6.7062e-01 -8.9877e-02  3.5474e-02  4.0090e-02  9.8321e-02  1.9817e-01]\n"]}],"source":["word = 'lawrence'\n","embedding_vector = embedding_index.get(word)\n","print(f'Embedding vector for \"{word}\": {embedding_vector}')"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":654,"status":"ok","timestamp":1714508902527,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"gGFi_9_YWYAd","outputId":"618937be-1e4e-47ef-c508-ebae0d829e09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precisión: 0.7125748502994012\n","Precisión (weighted): 0.6905424965525682\n","Exhaustividad (weighted): 0.7125748502994012\n","F1-score (weighted): 0.6924136374063784\n","Matriz de confusión:\n","[[ 40  90  45]\n"," [ 24 550  48]\n"," [ 26 103 243]]\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","\n","# Predicciones del conjunto de prueba\n","y_pred = clf.predict(X_test_vectors)\n","\n","# Calcular precisión\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Precisión:\", accuracy)\n","\n","# Calcular precisión\n","precision = precision_score(y_test, y_pred, average='weighted')\n","print(\"Precisión (weighted):\", precision)\n","\n","# Calcular exhaustividad\n","recall = recall_score(y_test, y_pred, average='weighted')\n","print(\"Exhaustividad (weighted):\", recall)\n","\n","# Calcular F1-score\n","f1 = f1_score(y_test, y_pred, average='weighted')\n","print(\"F1-score (weighted):\", f1)\n","\n","# Matriz de confusión\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"Matriz de confusión:\")\n","print(conf_matrix)\n"]},{"cell_type":"markdown","metadata":{"id":"GD8TPGefqS_4"},"source":["Los resultados obtenidos muestran un rendimiento moderado del modelo de clasificación, con una precisión del 71.26% y métricas relacionadas que indican una capacidad de predicción aceptable pero no excepcional. Si bien el modelo ha logrado identificar correctamente la mayoría de las muestras positivas en el conjunto de prueba, hay margen de mejora evidente. En este sentido, se plantea explorar una estrategia de transfer learning para potenciar el rendimiento del modelo. El transfer learning implica aprovechar el conocimiento aprendido por un modelo previamente entrenado en una tarea relacionada y aplicarlo a una nueva tarea."]},{"cell_type":"markdown","metadata":{"id":"JKf40z5tN7I-"},"source":["# Transfern learning - BERT"]},{"cell_type":"markdown","metadata":{},"source":["El modelo de transfer learning implementado sigue los siguientes pasos:\n","\n","1. Carga de Datos: Los datos de entrada, que consisten en frases (sentences) y etiquetas de sentimiento (labels), son cargados desde un dataframe o una estructura similar.\n","2. División de Datos: Los datos se dividen en conjuntos de entrenamiento y prueba utilizando la función train_test_split de scikit-learn. Esto asegura que el modelo pueda ser entrenado en una parte de los datos y evaluado en otra parte separada.\n","3. Tokenización: Se utiliza el tokenizador de BERT (Bidirectional Encoder Representations from Transformers) para convertir las frases de entrada en tokens y generar los correspondientes encodings necesarios para el modelo.\n","4. Creación del Dataset: Se define una clase SentimentDataset que hereda de Dataset de PyTorch. Esta clase se encarga de almacenar los encodings de las frases y las etiquetas de sentimiento y de proporcionar estos datos en el formato adecuado para el entrenamiento y la evaluación del modelo.\n","5. Carga del Modelo Pre-entrenado: Se carga un modelo pre-entrenado de BERT para clasificación de secuencias (BertForSequenceClassification) utilizando la función from_pretrained de la librería transformers de Hugging Face. Se especifica el número de etiquetas de clasificación, que en este caso son 3 (positivo, negativo, neutral).\n","6. Entrenamiento del Modelo: Se utiliza un DataLoader para cargar los datos de entrenamiento en lotes y se itera sobre estos lotes para realizar el entrenamiento del modelo. Durante el entrenamiento, se calcula la pérdida (loss) y se realizan las actualizaciones de los pesos del modelo utilizando el optimizador AdamW.\n","7. Evaluación del Modelo: Similar al entrenamiento, se utiliza un DataLoader para cargar los datos de prueba en lotes y se itera sobre estos lotes para realizar la evaluación del modelo. Se calculan las predicciones del modelo y se comparan con las etiquetas verdaderas para calcular la precisión del modelo utilizando la métrica de precisión (accuracy)."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":144874,"status":"ok","timestamp":1714504075928,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"vpERsb2NYm5d","outputId":"191f715f-d666-473c-8dad-e778dbe392b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7921300256629598\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","\n","# Cargar datos\n","data = data\n","sentences = data['Sentence'].tolist()\n","labels = data['Sentiment'].tolist()\n","\n","# Dividir datos en conjunto de entrenamiento y prueba\n","train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n","\n","# Convertir etiquetas a números enteros\n","label_map = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\n","train_labels = [label_map[label] for label in train_labels]\n","test_labels = [label_map[label] for label in test_labels]\n","\n","\n","# Tokenizar los datos\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n","test_encodings = tokenizer(test_sentences, truncation=True, padding=True)\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = SentimentDataset(train_encodings, train_labels)\n","test_dataset = SentimentDataset(test_encodings, test_labels)\n","\n","# Cargar modelo pre-entrenado\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 clases de sentimiento: positivo, negativo, neutral\n","\n","# Entrenamiento\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","model.train()\n","for batch in train_loader:\n","    optimizer.zero_grad()\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['labels'].to(device)\n","    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","    loss = outputs.loss\n","    loss.backward()\n","    optimizer.step()\n","\n","# Evaluación\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","model.eval()\n","all_preds = []\n","all_labels = []\n","for batch in test_loader:\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['labels'].tolist()\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","    preds = torch.argmax(logits, dim=1).tolist()\n","    all_preds.extend(preds)\n","    all_labels.extend(labels)\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(\"Accuracy:\", accuracy)\n"]},{"cell_type":"markdown","metadata":{},"source":["Con una precisión del 79%, el modelo de transfer learning basado en BERT ha superado significativamente tanto al modelo que utiliza embeddings pre-entrenados como al modelo basado en RNN.\n","\n","Comparado con el modelo de embeddings, que alcanzó una precisión del 71%, el modelo de transfer learning basado en BERT ha logrado un aumento notable del 8% en la precisión. Esto indica una mejora significativa en la capacidad del modelo para discernir y clasificar correctamente las expresiones de sentimiento en el texto en comparación con el enfoque anterior de embeddings pre-entrenados.\n","\n","Además, el rendimiento del modelo BERT es mucho mejor que el de la RNN, que solo logró una precisión del 54%. Esta diferencia de 25 puntos porcentuales resalta claramente la superioridad del enfoque de transfer learning basado en BERT sobre el enfoque anterior de RNN en términos de precisión en la tarea de análisis de sentimientos."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6501,"status":"ok","timestamp":1714504310716,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"bYx9R07Kaiiz","outputId":"cfed746f-2fdf-4c8b-d0ee-76a292e40173"},"outputs":[{"name":"stdout","output_type":"stream","text":["Modelo entrenado y tokenizador guardados en: ./model_transferlearning\n"]}],"source":["# Guardar el modelo entrenado\n","model_path = \"./model_transferlearning\"\n","\n","# Guardar el modelo y el tokenizador\n","model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)\n","\n","print(\"Modelo entrenado y tokenizador guardados en:\", model_path)\n"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1714507113569,"user":{"displayName":"Lawrence Javier M. V. K.","userId":"03782942042436835457"},"user_tz":-120},"id":"2w5y9PTHlN16","outputId":"1ffc7117-cbd3-41cb-e713-a7f7a0c91b7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7921300256629598\n","Precision: 0.8020280890262151\n","Recall: 0.7921300256629598\n","F1-score: 0.7934267354349172\n","Matriz de Confusión:\n","[[337   8  27]\n"," [ 39 103  33]\n"," [ 60  76 486]]\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","# Calcular métricas\n","accuracy = accuracy_score(all_labels, all_preds)\n","precision = precision_score(all_labels, all_preds, average='weighted')\n","recall = recall_score(all_labels, all_preds, average='weighted')\n","f1 = f1_score(all_labels, all_preds, average='weighted')\n","conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-score:\", f1)\n","print(\"Matriz de Confusión:\")\n","print(conf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"QruY4OZdqgIN"},"source":["Los resultados del modelo de transfer learning son muy positivos en general. Con una precisión del 79.21%, el modelo clasifica correctamente cerca del 79% de las muestras en el conjunto de prueba. La precisión del 80.20% indica su capacidad para identificar con precisión las muestras positivas. Además, el recall del 79.21% muestra su habilidad para recuperar la mayoría de las muestras positivas. El F1-score del 79.34% refleja un buen equilibrio entre precisión y recall. La matriz de confusión revela una clasificación correcta para la mayoría de las muestras, con pocos errores. En resumen, estos resultados sugieren que el modelo de transfer learning es efectivo y confiable para la tarea de análisis de sentimientos."]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMfPHw1tIvz7iaHiRAXpMGb","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
